{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"SlFU1XdscuBp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706180779835,"user_tz":-330,"elapsed":3669,"user":{"displayName":"Manoj Kumar","userId":"11111342431595334271"}},"outputId":"536df3cf-57e4-4029-c333-6d60f40fb040"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/github/filter_data\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","# drive.mount('/gdrive')\n","%cd /content/drive/MyDrive/github/filter_data"],"id":"SlFU1XdscuBp"},{"cell_type":"markdown","source":["## Fill Placeholders\n","\n"],"metadata":{"id":"sSmsJ7BsPy13"},"id":"sSmsJ7BsPy13"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5182,"status":"ok","timestamp":1706180785013,"user":{"displayName":"Manoj Kumar","userId":"11111342431595334271"},"user_tz":-330},"id":"VGk6d7v7dYl5","outputId":"1cb996c2-dc00-4a14-f47c-c99a8c7502ab"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: indic-num2words in /usr/local/lib/python3.10/dist-packages (1.2.0)\n"]}],"source":["!pip install indic-num2words"],"id":"VGk6d7v7dYl5"},{"cell_type":"code","source":["import dataprep_utils\n","import importlib\n","importlib.reload(dataprep_utils)\n","lang_list = ['hi', 'en']"],"metadata":{"id":"Kp9I5L5bRTXh"},"id":"Kp9I5L5bRTXh","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HiG9766a1f_s"},"outputs":[],"source":["# for lang in lang_list:\n","#   for i in range(10):\n","#     dataprep_utils.fill_placeholders(lang)"],"id":"HiG9766a1f_s"},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"swvHOtX-f7eV"},"id":"swvHOtX-f7eV"},{"cell_type":"markdown","source":["### Get manually created data"],"metadata":{"id":"IPH3XM_5mBYz"},"id":"IPH3XM_5mBYz"},{"cell_type":"code","source":["import os\n","import pandas as pd\n","\n","def get_training_data(lang_list):\n","    df = pd.DataFrame()\n","    # lang = 'en'\n","    all_data = pd.DataFrame()\n","    for lang in lang_list:\n","        read_dir = os.path.join(os.getcwd(),'formatted_data', lang)\n","        files = os.listdir(read_dir)\n","        data_files = [file for file in files if file.endswith(\".csv\")]\n","\n","        for file in data_files:\n","            file_df = pd.read_csv(f'{os.path.join(read_dir, file)}')\n","            df = pd.concat([df, file_df], ignore_index=True)\n","\n","    return df"],"metadata":{"id":"tapidqv3gAeW"},"id":"tapidqv3gAeW","execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = get_training_data(lang_list)\n","data = data.rename(columns={\"sentence\": \"Text\", \"intent\": \"Label\", 'language': 'lang'})\n","data.tail()"],"metadata":{"id":"XxFE-LtggmDx"},"id":"XxFE-LtggmDx","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training utilities"],"metadata":{"id":"Ff3cBDaPmWEC"},"id":"Ff3cBDaPmWEC"},{"cell_type":"code","source":["# !pip install transformers\n","# !pip install sentencepiece\n","# !pip install transformers[torch]\n","# # !pip install accelerate -U"],"metadata":{"id":"GKbOcTf_m3sf"},"id":"GKbOcTf_m3sf","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import argparse\n","import os\n","import pickle\n","import numpy as np\n","import pandas as pd\n","import torch\n","\n","# import utils\n","# import importlib\n","# importlib.reload(utils)\n","\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","from sklearn.model_selection import train_test_split\n","from transformers import (\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","    EarlyStoppingCallback,\n","    Trainer,\n","    TrainingArguments,\n",")\n","\n","os.environ[\"WANDB_DISABLED\"] = \"true\"\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","torch.cuda.set_device(0)\n","torch.cuda.empty_cache()\n","\n","MODEL = \"ai4bharat/indic-bert\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","\n","\n","def tokenize_data(texts):\n","    tokenized_inputs = tokenizer(\n","        texts, padding=\"max_length\", max_length=512, truncation=True\n","    )\n","    return tokenized_inputs\n","\n","\n","def compute_metrics(p):\n","    pred, labels = p\n","    pred = np.argmax(pred, axis=1)\n","\n","    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n","    recall = recall_score(y_true=labels, y_pred=pred, average=\"micro\")\n","    precision = precision_score(y_true=labels, y_pred=pred, average=\"micro\")\n","    f1 = f1_score(y_true=labels, y_pred=pred, average=\"micro\")\n","\n","    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n","\n","\n","def train(data):\n","    df = data\n","    df = df.dropna()\n","\n","    labels = sorted(list(set(df[\"Label\"])))\n","    labels_to_ids = {k: v for v, k in enumerate(sorted(labels))}\n","\n","    X = list(df[\"Text\"])\n","    y = list(df[\"Label\"])\n","    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n","    X_train_tokenized = tokenizer(\n","        X_train, padding=True, truncation=True, max_length=512\n","    )\n","    X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)\n","\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        MODEL, num_labels=len(labels)\n","    )\n","    train_dataset = utils.Dataset(X_train_tokenized, y_train, labels_to_ids)\n","    val_dataset = utils.Dataset(X_val_tokenized, y_val, labels_to_ids)\n","    output_dir = os.path.join(os.getcwd(),'models')\n","\n","    train_args = TrainingArguments(\n","        output_dir=output_dir,\n","        evaluation_strategy=\"steps\",\n","        eval_steps=500,\n","        per_device_train_batch_size=32,\n","        per_device_eval_batch_size=32,\n","        num_train_epochs=3,\n","        seed=0,\n","        save_steps=500,\n","        load_best_model_at_end=True,\n","    )\n","    trainer = Trainer(\n","        model=model,\n","        args=train_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=val_dataset,\n","        compute_metrics=compute_metrics,\n","        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n","    )\n","    with open(os.path.join(output_dir, \"labels-dict.pkl\"), \"wb\") as f:\n","        pickle.dump(labels_to_ids, f)\n","\n","    train_metrics = trainer.train()\n","    eval_metrics = trainer.evaluate()\n","\n","    print(train_metrics)\n","    print(eval_metrics)"],"metadata":{"id":"-1J7M5Wxf695"},"id":"-1J7M5Wxf695","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train the model"],"metadata":{"id":"P0vek2MtmdBi"},"id":"P0vek2MtmdBi"},{"cell_type":"code","source":["# train(data)"],"metadata":{"id":"qDsR0n03pJAb"},"id":"qDsR0n03pJAb","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Get GPT generated data"],"metadata":{"id":"nd78KOnlmi85"},"id":"nd78KOnlmi85"},{"cell_type":"code","source":["lang_list = ['hi', 'en']\n","def get_gpt_data(lang_list):\n","  df = pd.read_csv(os.path.join(os.getcwd(),'gpt_data', 'all-lang-all.csv'))\n","  df = df[df['lang'].isin(lang_list)]\n","  return df"],"metadata":{"id":"hxCYYJkt58fq"},"id":"hxCYYJkt58fq","execution_count":null,"outputs":[]},{"cell_type":"code","source":["gpt_data = get_gpt_data(lang_list)"],"metadata":{"id":"NsMV8_XY6r0J"},"id":"NsMV8_XY6r0J","execution_count":null,"outputs":[]},{"cell_type":"code","source":["gpt_data.shape"],"metadata":{"id":"29E1jRSeqohm","executionInfo":{"status":"ok","timestamp":1706181108833,"user_tz":-330,"elapsed":1049,"user":{"displayName":"Manoj Kumar","userId":"11111342431595334271"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"05d26376-9dab-4c1f-dc24-a73aed011179"},"id":"29E1jRSeqohm","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(49499, 3)"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":[],"metadata":{"id":"5WwiVj6P5WjC"},"id":"5WwiVj6P5WjC","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","\n","import numpy as np\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer\n","from utils import Dataset\n","\n","MODEL = \"ai4bharat/indic-bert\"\n","\n","\n","def softmax(x):\n","    e_x = np.exp(x)\n","    return e_x / np.sum(e_x, axis=1, keepdims=True)\n","\n","\n","class IntentRecognizer:\n","    def __init__(self, model_path, label_dict_pkl, conf_threshold):\n","        with open(label_dict_pkl, \"rb\") as f:\n","            self.labels_to_ids = pickle.load(f)\n","        self.ids_to_labels = {\n","            intent_id: intent_label\n","            for intent_label, intent_id in self.labels_to_ids.items()\n","        }\n","        self.tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","        self.model = AutoModelForSequenceClassification.from_pretrained(\n","            model_path, num_labels=len(self.labels_to_ids)\n","        )\n","        self.test_trainer = Trainer(self.model)\n","        self.conf_threshold = conf_threshold\n","\n","    def predict(self, sentence):\n","        sentence = [sentence]\n","        sentence_tokenized = self.tokenizer(\n","            sentence, padding=True, truncation=True, max_length=512\n","        )\n","        model_in = Dataset(sentence_tokenized)\n","        raw_pred, _, _ = self.test_trainer.predict(model_in)\n","        probs = softmax(raw_pred)\n","        y_pred = np.argmax(probs, axis=1)[0]\n","        pred_prob = np.max(probs)\n","\n","        orig_pred = self.ids_to_labels[y_pred]\n","        # pred = self.ids_to_labels[y_pred]\n","        # if pred_prob < self.conf_threshold:\n","        #     pred = \"unknown\"\n","        return orig_pred, pred_prob"],"metadata":{"id":"7T0jlxRbD9pD"},"id":"7T0jlxRbD9pD","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","# from intent_recognizer import IntentRecognizer\n","from sklearn.metrics import classification_report\n","from tqdm import tqdm\n","\n","\n","def test(data):\n","    df = data\n","    df = df.dropna()\n","    model_path = os.path.join(os.getcwd(),'models', 'jan25')\n","\n","    intent_recognizer = IntentRecognizer(\n","        model_path, os.path.join(os.getcwd(),'models', 'labels-dict.pkl'), conf_threshold=0.85\n","    )\n","\n","    true_intent = list()\n","    pred_intent = list()\n","    pred_probability = list()\n","\n","    for i, row in tqdm(df.iterrows()):\n","        true_intent.append(row[\"Label\"])\n","        intent, pred_prob = intent_recognizer.predict(row[\"Text\"])\n","        pred_intent.append(intent)\n","        pred_probability.append(pred_prob)\n","\n","    df[\"Predicted Intent\"] = pred_intent\n","    df[\"Predicted Prob\"] = pred_probability\n","    print(classification_report(true_intent, pred_intent))\n","    df.to_csv('gpt_data.csv', index=False)\n","    return df\n"],"metadata":{"id":"Zwp8zERgEH0z"},"id":"Zwp8zERgEH0z","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# filtered_data = test(gpt_data)"],"metadata":{"id":"qOitDGDGNS3p"},"id":"qOitDGDGNS3p","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}